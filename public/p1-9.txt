Paper1 Practical 9
Apply K-Means clustering and Hierarchical clustering on the following dataset. Display the elbow chart to detect number of clusters. Also display cluster wise data.

##K- Means Clustering
import warnings
warnings.simplefilter("ignore")
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

df=pd.read_csv("D:/Downloads/Mall_Customers.csv")
df

df.head()
df.columns

X=df.iloc[:,[3,4]].values
X

#from sklearn.cluster import KMeans
#KMeans

#find number of clusters
from sklearn.cluster import KMeans 
wcss=[]
for i in range(1,11):
  kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)

wcss

for i in range(0,10):
  print(i+1,wcss[i])

plt.plot(range(1,11),wcss)
plt.title("Elbow method")
plt.xlabel("Number of clusters")
plt.ylabel("WCSS")
plt.show()

# we get k = 5
kmeans= KMeans(n_clusters=5,init='k-means++',random_state=42)
y_kmeans=kmeans.fit_predict(X)
y_kmeans

#visualization
plt.scatter(X[y_kmeans==0,0], X[y_kmeans==0,1],s=100,c='red',label='Cluster1')
plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='Cluster2')
plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3')
plt.scatter(X[y_kmeans==3,0],X[y_kmeans==3,1],s=100,c='cyan',label='Cluster4')
plt.scatter(X[y_kmeans==4,0],X[y_kmeans==4,1],s=100,c='magenta',label='Cluster5')
plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,color='yellow',label='Centroid')
plt.title("Visualization")
plt.xlabel("annual income")
plt.ylabel("spending score")
plt.legend()
plt.show()

##Hierarchical Clustering

import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X,method='ward'))
plt.title("Dendrogram")
plt.xlabel("Cusatomers")
plt.ylabel("Euclidean distance")
plt.show()

from sklearn.cluster import AgglomerativeClustering
hc=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='ward')
y_hc= hc.fit_predict(X)

y_hc

#visualization
plt.scatter(X[y_hc==0,0], X[y_hc==0,1],s=100,c='red',label='Cluster1')
plt.scatter(X[y_hc==1,0], X[y_hc==1,1],s=100,c='blue',label='Cluster2')
plt.scatter(X[y_hc==2,0], X[y_hc==2,1],s=100,c='green',label='Cluster3')
plt.scatter(X[y_hc==3,0], X[y_hc==3,1],s=100,c='cyan',label='Cluster4')
plt.scatter(X[y_hc==4,0], X[y_hc==4,1],s=100,c='magenta',label='Cluster5')
plt.title("Visualization")
plt.xlabel("annual income")
plt.ylabel("spending score")
plt.legend()
plt.show()

from sklearn.cluster import AgglomerativeClustering
hc=AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='ward')
y_hc= hc.fit_predict(X)
y_hc
